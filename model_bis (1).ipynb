{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad733fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_parts_with_labels_from_documents():\n",
    "    data = []\n",
    "    doc_counter = 0\n",
    "    \n",
    "    # Parcours de tous les fichiers dans le dossier de documents\n",
    "    for filename in os.listdir():\n",
    "    \n",
    "        if filename.endswith(\".txt\"):  # Assurez-vous que le fichier est un fichier texte\n",
    "            with open(os.path.join(filename), 'r', encoding='utf-8') as file:\n",
    "                doc_content = file.read()\n",
    "                \n",
    "                # Extraire les parties avec leurs labels\n",
    "                pattern = r'<para>\\s*(\\d+)\\s+(.*?)<\\\\para>'\n",
    "                matches = re.findall(pattern, doc_content, re.DOTALL)\n",
    "                \n",
    "                # Ajouter les données extraites à la liste\n",
    "                for match in matches:\n",
    "                    label = match[0]\n",
    "                    part = match[1].strip()\n",
    "                    data.append((part, int(label), doc_counter))\n",
    "                    \n",
    "            doc_counter += 1  # Incrémenter le numéro du document\n",
    "    \n",
    "    # Créer un DataFrame Pandas à partir des données\n",
    "    df = pd.DataFrame(data, columns=['Contenu du paragraphe', 'Label', 'Numéro du document'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Exemple d'utilisation avec un dossier contenant des documents\n",
    "\n",
    "# Extraire les parties et les labels de tous les documents\n",
    "df = extract_parts_with_labels_from_documents()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be21150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913995e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['Contenu du paragraphe'].values.tolist()\n",
    "sentiments = df['Label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32c98068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"Label\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e8c5c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"Label\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e6dd11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2677595628415301"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49/183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbe74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd85588",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "MAX_LEN = 128\n",
    "batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c5688",
   "metadata": {},
   "source": [
    "# Spliting Training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d755a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6890382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Contenu du paragraphe', 'Label', 'Numéro du document'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03509c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "text = df['Contenu du paragraphe'].to_list()\n",
    "labels = df['Label'].to_list()\n",
    "\n",
    "#user tokenizer to convert sentences into tokenizer\n",
    "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ede563",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=42, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4592c9c",
   "metadata": {},
   "source": [
    "# CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7a5f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3baa95c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b8a1e",
   "metadata": {},
   "source": [
    "# Defining the parameters and metrics to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e54c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seydinaousmaneniang/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802088c",
   "metadata": {},
   "source": [
    "# Training and evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d8171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                              | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6157316186211326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|███████▌                              | 1/5 [01:29<05:58, 89.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6458333333333333\n",
      "Train loss: 0.5292431901801716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|███████████████▏                      | 2/5 [02:56<04:24, 88.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6458333333333333\n",
      "Train loss: 0.5035618137229573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████████████████████▊               | 3/5 [04:22<02:54, 87.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6458333333333333\n",
      "Train loss: 0.43375999277288263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|██████████████████████████████▍       | 4/5 [05:54<01:28, 88.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7395833333333333\n",
      "Train loss: 0.2954600710760463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████| 5/5 [07:26<00:00, 89.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting if we want to visualize training evolution per epochs after the training process\n",
    "train_loss_set = []\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    # Tracking variables for training\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to device CPU or GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Get loss value\n",
    "        loss = outputs[0]\n",
    "        # Add it to train loss list\n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Tracking variables for validation\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Validation of the model\n",
    "    model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to device CPU or GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss, logits = outputs[:2]\n",
    "    \n",
    "        # Move logits and labels to CPU if GPU is used\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913859cd",
   "metadata": {},
   "source": [
    "# Test the model on an unseen texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93cb5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [\"présence d'un stimulateur cardiaque implanté dans le corps\", \n",
    "           \" peacemaker  implanté dans le corps\",\n",
    "            \"radiographie cardiaque\"\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1da63f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in comments]\n",
    "# Pad the resulted encoded comments\n",
    "tokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks \n",
    "attention_masks = []\n",
    "for seq in tokenized_comments_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "prediction_inputs = torch.tensor(tokenized_comments_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83a2b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_pred = []\n",
    "with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy() \n",
    "    flat_pred.extend(np.argmax(logits, axis=1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58fcbd18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:  présence d'un stimulateur cardiaque implanté dans le corps\n",
      "Label 1\n",
      "Comment:   peacemaker  implanté dans le corps\n",
      "Label 1\n",
      "Comment:  radiographie cardiaque\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(flat_pred)):\n",
    "    print('Comment: ', comments[i])\n",
    "    print('Label', flat_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a7edc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(comments):\n",
    "    tokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in comments]\n",
    "    # Pad the resulted encoded comments\n",
    "    tokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # Create attention masks \n",
    "    attention_masks = []\n",
    "    for seq in tokenized_comments_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    prediction_inputs = torch.tensor(tokenized_comments_ids)\n",
    "    prediction_masks = torch.tensor(attention_masks)\n",
    "    flat_pred = []\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy() \n",
    "        flat_pred.extend(np.argmax(logits, axis=1).flatten())\n",
    "    return flat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72030bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_paragraphes(texte):\n",
    "    paragraphes = re.split(r'\\n\\s*\\n', texte)  # Sépare le texte en paragraphes\n",
    "    paragraphes_pertinents = []\n",
    "    for paragraphe in paragraphes:\n",
    "        if re.search(r'\\b(cardiaque|stimulateur\\s+cardiaque)\\b', paragraphe, re.IGNORECASE):\n",
    "            paragraphes_pertinents.append(paragraphe)\n",
    "    return paragraphes_pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_report_analysis(text):\n",
    "    paragraphs = extraire_paragraphes(text)\n",
    "    pred = model(paragraphs)\n",
    "    return max(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
